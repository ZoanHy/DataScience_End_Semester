{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_links(url):\n",
    "    soup = get_soup(url)\n",
    "    aitems = []\n",
    "    for h3 in soup.find_all(\"h3\"):\n",
    "        try:\n",
    "            a = h3.a[\"href\"]\n",
    "        except:\n",
    "            KeyError\n",
    "        aitems.append(a)\n",
    "    return aitems\n",
    "\n",
    "\n",
    "all_links1 = [\n",
    "    f\"https://donghoduyanh.com/dong-ho-nam-dc594-page{idx_page}.html\"\n",
    "    for idx_page in range(1, 260)\n",
    "]\n",
    "all_links2 = [\n",
    "    f\"https://donghoduyanh.com/dong-ho-nu-dc595-page{idx_page}.html\"\n",
    "    for idx_page in range(1, 120)\n",
    "]\n",
    "\n",
    "all_links = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=25) as executor:\n",
    "    futures = [executor.submit(extract_links, url) for url in all_links1 + all_links2]\n",
    "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "all_links = [link for sublist in results for link in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11320"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_links = list(set(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        response = session.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "# def getAll_ThuongHieu(url):\n",
    "soup = get_soup(\"https://donghoduyanh.com/thuong-hieu.html\")\n",
    "content = soup.find(\"div\", class_=\"module_manu\")\n",
    "item = content.find_all(\"a\")\n",
    "ThuongHieus = []\n",
    "for i in item:\n",
    "    ThuongHieus.append(i[\"title\"].lower())\n",
    "ThuongHieus.append(\"freelook\")\n",
    "ThuongHieus.append(\"eterna\")\n",
    "ThuongHieus.append(\"charriol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_thuonghieu_match(thuonghieus, S):\n",
    "    for thuonghieu in thuonghieus:\n",
    "        if thuonghieu.strip() in S.lower():\n",
    "            return thuonghieu.upper()\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_product_info(url):\n",
    "    soup = get_soup(url)\n",
    "    details = {}\n",
    "    nameAndtype = soup.find(\"h1\", class_=\"bk-product-name\")\n",
    "    ds = nameAndtype.getText().strip().split(\" \")\n",
    "    type = ds[-1].strip()\n",
    "    name = (\n",
    "        nameAndtype.getText()\n",
    "        .replace(type, \"\")\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\"đồng hồ\", \"\")\n",
    "        .replace(\"nam\", \"\")\n",
    "        .replace(\"nữ\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    thuonghieu = find_thuonghieu_match(ThuongHieus, name)\n",
    "    price = (\n",
    "        soup.find_all(\"div\", class_=\"number_price_current\")[0]\n",
    "        .text.replace(\"₫\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    if thuonghieu != None:\n",
    "        content = soup.find(\"div\", class_=\"table-condensed compare_table\")\n",
    "        rows = content.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            text = row.getText().replace(\"\\t\", \"\").replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "            if len(text.split(\":\")) == 2:\n",
    "                key, value = text.split(\":\")\n",
    "                details[key.lower().strip()] = value.strip()\n",
    "        a = {\n",
    "            \"Thương hiệu\": thuonghieu,\n",
    "            \"Giá tiền\": price,\n",
    "            \"Mã sản phẩm\": type,\n",
    "            **details,\n",
    "        }\n",
    "        # print(a)\n",
    "        return a\n",
    "\n",
    "\n",
    "def crawl_data_from_web(all_links):\n",
    "    data_frame_watchs = pd.DataFrame()\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        results = executor.map(get_product_info, all_links)\n",
    "        for result in results:\n",
    "            data_frame_watchs = data_frame_watchs.append(result, ignore_index=True)\n",
    "    return data_frame_watchs\n",
    "\n",
    "\n",
    "data_frame_watchs = crawl_data_from_web(all_links[0:2999])\n",
    "\n",
    "data_frame_watchs = data_frame_watchs.dropna(\n",
    "    axis=1, thresh=len(data_frame_watchs) * 0.4\n",
    ")\n",
    "\n",
    "data_frame_watchs.to_csv(\"../main/raw_data/duy_anh.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
